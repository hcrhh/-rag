import os
import re
import json
import logging
import aiohttp
import numpy as np
import asyncio
from functools import lru_cache
from tenacity import retry, wait_exponential, stop_after_attempt
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from typing import Dict, List, Union, BinaryIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from typing import AsyncGenerator, Dict, List, Union
from FlagEmbedding import FlagReranker
from pathlib import Path
from langchain_core.runnables import RunnableLambda
import faiss
import sys
from FlagEmbedding import FlagReranker
# åœ¨newrag.pyä¸­æ·»åŠ æµ‹è¯•ä»£ç 
from sentence_transformers import util
from sentence_transformers import SentenceTransformer
logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.INFO)
# ======== æ–°å¢æ™ºèƒ½æ—¥å¿—è¿‡æ»¤å™¨ ========
# æ–°å¢è°ƒè¯•æ§åˆ¶
DEBUG_MODE = True  # è®¾ä¸ºFalseå…³é—­è¯¦ç»†æ—¥å¿—

if DEBUG_MODE:
    logging.getLogger().setLevel(logging.DEBUG)
    logging.debug("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
else:
    logging.getLogger().setLevel(logging.INFO)





# æ–°å¢æœ¯è¯­ç»Ÿä¸€å‡½æ•°

def normalize_medical_terms(text: str) -> str:
    replacements = {
        "è€äºº": "è€å¹´æ‚£è€…",
        "é¥±èƒ€æ„Ÿ": "è…¹èƒ€",
        "3å¤©": "ä¸‰æ—¥",
        "è‚šå­ç—›": "è…¹ç—›",
        "å¿ƒæ…Œ": "å¿ƒæ‚¸"
    }
    for informal, formal in replacements.items():
        text = text.replace(informal, formal)
    return text



class MedicalLogFilter(logging.Filter):
    def filter(self, record):
        # è¿‡æ»¤ç‰¹å®šè­¦å‘Šæ¨¡å¼
        if "ç©ºchoicesåˆ—è¡¨" in record.getMessage():
            # è§£ææ—¥å¿—å‚æ•°
            chunk = record.args[0] if record.args else {}
            # å¿½ç•¥æ¡ä»¶ï¼šæœ‰æœ‰æ•ˆusageä¸”æ˜¯æµå¼åˆ†å—
            if isinstance(chunk, dict):
                if chunk.get('object') == 'chat.completion.chunk' and \
                   chunk.get('usage', {}).get('completion_tokens', 0) > 0:
                    return False
        return True
#åˆ›å»ºå¼•æ“
class MedicalEngine:
    def __init__(self, query_func):
        self.query_func = query_func
        self.index = self._initialize_index()
        self.embedder = HuggingFaceEmbeddings(  # âœ… æ–°å¢
            model_name="shibing624/text2vec-base-chinese"
        )

    def _initialize_index(self):
        """åŠ è½½é¢„æ„å»ºçš„FAISSç´¢å¼•ï¼ˆå·²ä¿®æ­£è·¯å¾„ï¼‰"""
        try:
            # å¤šçº§çˆ¶ç›®å½•è·³è½¬ï¼šnew2 -> medical-rag -> Rag1 -> data/medical_faiss
            index_path = (
                    Path(__file__).resolve().parent  # new2ç›®å½•
                    .parent.parent  # ä¸Šæº¯åˆ°Rag1ç›®å½•
                    / "data"
                    / "medical_faiss"
                    / "index.faiss"
            )

            # éªŒè¯è·¯å¾„æœ‰æ•ˆæ€§
            if not index_path.exists():
                raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨äºé¢„æœŸè·¯å¾„: {index_path}")

            logging.info(f"âœ… æ­£åœ¨åŠ è½½FAISSç´¢å¼•ï¼š{index_path}")
            return faiss.read_index(str(index_path))

        except Exception as e:
            logging.critical(f"ğŸ”´ ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
            raise

    async def aquery(self, question: str) -> dict:
        """å¢å¼ºç‰ˆæŸ¥è¯¢æ¥å£ï¼Œè¿”å›åŒ…å«åŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬"""
        full_response = {
            "answer": "",
            "sources": [],  # ä¿ç•™åŸå§‹ç»“æ„åŒ–æ¥æºä¿¡æ¯
            "context_texts": [],  # æ–°å¢ï¼šåŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            "error": None
        }

        try:
            # 1. ç”Ÿæˆé—®é¢˜åµŒå…¥å‘é‡
            embedding = self.embedder.embed_query(question)
            embedding = np.array(embedding).astype('float32').reshape(1, -1)

            # 2. æ‰§è¡ŒFAISSæœç´¢
            distances, indices = self.index.search(embedding, k=5)
            logging.info(f"FAISSæœç´¢ç»“æœ: è·ç¦»{distances[0]}, ç´¢å¼•{indices[0]}")

            # 3. æ”¶é›†æ‰€æœ‰ä¸Šä¸‹æ–‡æ–‡æœ¬
            context_texts = []

            # 4. å¤„ç†æµå¼å“åº”
            answer_parts = []
            async for chunk in self.query_func(question):
                if "delta" in chunk:
                    answer_parts.append(chunk["delta"])
                elif "answer" in chunk:
                    answer_parts.append(chunk["answer"])

                    # å¤„ç†æ¥æºä¿¡æ¯
                    sources = []
                    for src in chunk.get("sources", []):
                        # ä¿ç•™ç»“æ„åŒ–æ¥æºä¿¡æ¯
                        source_entry = {
                            "title": src.get("title", "æœªçŸ¥æ–‡çŒ®"),
                            "pages": src.get("pages", ["N/A"]),
                            "content": src.get("content", "")  # æ–°å¢å†…å®¹å­—æ®µ
                        }
                        sources.append(source_entry)

                        # æ”¶é›†ä¸Šä¸‹æ–‡æ–‡æœ¬
                        if "content" in src:
                            context_texts.append(src["content"])
                        elif "text" in src:
                            context_texts.append(src["text"])

                    full_response["sources"] = sources

                # ç›´æ¥ä»chunkæ”¶é›†ä¸Šä¸‹æ–‡
                if "context" in chunk:
                    context_texts.append(chunk["context"])

            full_response["answer"] = "".join(answer_parts).strip()
            full_response["context_texts"] = context_texts  # è®¾ç½®ä¸Šä¸‹æ–‡æ–‡æœ¬

            # å¦‚æœæ²¡æœ‰è·å–åˆ°å›ç­”ï¼Œè®¾ç½®é»˜è®¤å€¼
            if not full_response["answer"]:
                full_response["answer"] = "æœªæ‰¾åˆ°ç›¸å…³åŒ»å­¦è¯æ®"
                logging.warning(f"é—®é¢˜'{question[:30]}...'æœªè·å¾—æœ‰æ•ˆå›ç­”")

            # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»å›ç­”ä¸­æå–
            if not full_response["context_texts"] and full_response["answer"]:
                import re
                context_matches = re.findall(r'â–²æ¥æº\d+ï¼š(.*?)(?=â–²|$)', full_response["answer"])
                if context_matches:
                    full_response["context_texts"] = [match.strip() for match in context_matches]

        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤„ç†å¼‚å¸¸: {str(e)}"
            logging.error(error_msg, exc_info=True)
            full_response["error"] = error_msg

        return full_response

    async def _cleanup_async_resources(self):
        """æ¸…ç†å¯èƒ½æ®‹ç•™çš„å¼‚æ­¥è¿æ¥"""
        if hasattr(self, 'http_session') and self.http_session:
            await self.http_session.close()
            self.http_session = None






#æ’åºå™¨ç±»
class MedicalReranker:
    def __init__(self):
        self.model = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)
        self.semantic_model = SentenceTransformer('shibing624/text2vec-base-chinese')
####+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    def _debug_output(self, query: str, sorted_docs: list):
        """è°ƒè¯•ç”¨é‡æ’åºè¾“å‡º"""
        logging.debug("\n=== é‡æ’åºè°ƒè¯• ===")
        logging.debug(f"æŸ¥è¯¢: {query}")
        for i, (doc, score) in enumerate(sorted_docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            logging.debug(f"æ–‡æ¡£{i + 1} [å¾—åˆ†:{score:.2f}]: {doc.page_content[:100]}...")
            logging.debug(f"å…ƒæ•°æ®: {dict(list(doc.metadata.items())[:3])}...")  # æ˜¾ç¤ºå‰3ä¸ªå…ƒæ•°æ®
####++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    async def rerank(self, query: str, docs: list) -> list:
        """æ··åˆé‡æ’åºç­–ç•¥"""
        try:
            # 1. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
            query_embedding = self.semantic_model.encode(query)
            doc_embeddings = self.semantic_model.encode([d.page_content for d in docs])
            semantic_scores = util.cos_sim(query_embedding, doc_embeddings)[0]

            # 2. ç²¾ç»†æ’åºè®¡ç®—
            pairs = [(query, doc.page_content) for doc in docs]
            fine_scores = self.model.compute_score(pairs)

            # 3. æ··åˆè¯„åˆ† (70%è¯­ä¹‰ + 30%ç²¾ç»†)
            combined_scores = [
                0.7 * semantic.item() + 0.3 * fine
                for semantic, fine in zip(semantic_scores, fine_scores)
            ]

            # 4. æŒ‰ç±»å‹å¢å¼º (é—®é¢˜å—ä¼˜å…ˆ)
            sorted_docs = []
            for doc, score in zip(docs, combined_scores):
                if doc.metadata.get('chunk_type') == 'question':
                    score *= 1.2
                doc.metadata['rerank_score'] = float(score)
                sorted_docs.append((doc, score))

            # æœ€ç»ˆæ’åº
            sorted_docs.sort(key=lambda x: x[1], reverse=True)

            if DEBUG_MODE:
                self._debug_output(query, sorted_docs)

            return [doc for doc, _ in sorted_docs]

        except Exception as e:
            logging.error(f"é‡æ’åºå¤±è´¥: {str(e)}", exc_info=True)
            return sorted(docs, key=lambda x: x.metadata.get('similarity_score', 0), reverse=True)

    async def debug_rerank(self, query: str, docs: list):
        """è°ƒè¯•ç”¨é‡æ’åºè¾“å‡º"""
        if DEBUG_MODE:
            logging.debug("=== é‡æ’åºè°ƒè¯• ===")
            logging.debug(f"æŸ¥è¯¢: {query}")
            for i, doc in enumerate(docs):
                logging.debug(f"æ–‡æ¡£{i + 1}: {doc.page_content[:100]}...")
                logging.debug(f"å…ƒæ•°æ®: {doc.metadata}")


class MedicalInputProcessor:
    def __init__(self, embeddings):
        self.embeddings = embeddings
        # ç»Ÿä¸€ä½¿ç”¨QAç»“æ„çš„åˆ†å‰²ç­–ç•¥
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=768,
            chunk_overlap=64,
            separators=["\né—®é¢˜ï¼š", "\nå›ç­”ï¼š", "\né—®ï¼š", "\nç­”ï¼š", "\n\n"],
            keep_separator=True
        )

    async def process(self, content: Union[str, bytes]) -> FAISS:
        """å¤„ç†è¾“å…¥å¹¶ç¡®ä¿ä¸ä¸»å‘é‡åº“ç›¸åŒåˆ†å—ç­–ç•¥"""
        text = await self._parse_content(content)
        # é¢„å¤„ç†æ–‡æœ¬ä»¥åŒ¹é…QAç»“æ„
        processed_text = self._preprocess_text(text)
        docs = self.splitter.create_documents([processed_text[:50000]])

        # ä¸ºæ¯ä¸ªå—æ·»åŠ æ ‡å‡†å…ƒæ•°æ®
        for doc in docs:
            doc.metadata.update({
                'is_core': False,
                'title': 'ç”¨æˆ·ä¸Šä¼ å†…å®¹',
                'chunk_type': self._identify_chunk_type(doc.page_content)
            })
        return FAISS.from_documents(docs, self.embeddings)

    def _preprocess_text(self, text: str) -> str:
        """å°†æ™®é€šæ–‡æœ¬è½¬æ¢ä¸ºQAæ ¼å¼ä»¥åŒ¹é…åˆ†å—ç­–ç•¥"""
        # 1. æ ‡å‡†åŒ–åŒ»å­¦æœ¯è¯­
        text = normalize_medical_terms(text)

        # 2. è‡ªåŠ¨è¯†åˆ«é—®é¢˜å¹¶æ·»åŠ æ ‡è®°
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        processed_lines = []
        for sent in sentences:
            if sent.strip() and len(sent) > 10:  # ç®€å•çš„é—®é¢˜è¯†åˆ«å¯å‘å¼è§„åˆ™
                if any(q_word in sent for q_word in ['å—', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ˜¯å¦']):
                    processed_lines.append(f"\né—®é¢˜ï¼š{sent.strip()}")
                else:
                    processed_lines.append(f"\nå›ç­”ï¼š{sent.strip()}")
        return "\n".join(processed_lines)

    def _identify_chunk_type(self, content: str) -> str:
        """è¯†åˆ«å—ç±»å‹ç”¨äºåç»­å¤„ç†"""
        if content.startswith(("\né—®é¢˜ï¼š", "\né—®ï¼š")):
            return "question"
        elif content.startswith(("\nå›ç­”ï¼š", "\nç­”ï¼š")):
            return "answer"
        return "other"


def _enhance_question_structure(question: str) -> str:
    """å¢å¼ºé—®é¢˜ç»“æ„ä»¥åŒ¹é…å‘é‡åº“æ ¼å¼"""
    if not question.startswith(("\né—®é¢˜ï¼š", "\né—®ï¼š")):
        # æ·»åŠ é—®é¢˜å‰ç¼€å¹¶ç§»é™¤å·²æœ‰æ¢è¡Œ
        question = re.sub(r'^\s*[\r\n]+', '', question)
        return f"\né—®é¢˜ï¼š{question}"
    return question





# åº”ç”¨è¿‡æ»¤å™¨
logging.getLogger("root").addFilter(MedicalLogFilter())
class MedicalQwenClient:
    def __init__(self):
        self.api_key = os.getenv("SILICON_API_KEY")
        assert self.api_key, "æœªè®¾ç½®SILICON_API_KEYç¯å¢ƒå˜é‡"
        self.base_url = "https://api.siliconflow.cn/v1"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    async def test_connection(self):
        async with aiohttp.ClientSession() as session:
            async with session.get(
                    f"{self.base_url}/models",
                    headers=self.headers,
                    timeout=10
            ) as response:
                return await response.json()

    # æµ‹è¯•ä»£ç 


    @retry(wait=wait_exponential(multiplier=1, min=2, max=10),
           stop=stop_after_attempt(3))
    async def generate_stream(self, messages: List[Dict]) -> AsyncGenerator[str, None]:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": "Qwen/QwQ-32B",  # ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 1024,
                        "top_p": 0.9,  # å¢åŠ å¤šæ ·æ€§
                        "presence_penalty": 0.5,  # å‡å°‘é‡å¤
                        "stream": True,
                        "stream_options": {
                            "include_usage": True,
                            "heartbeat_interval": 15
                        }
                    },
                    timeout=aiohttp.ClientTimeout(
                        total=300,  # æ€»è¶…æ—¶å»¶é•¿è‡³5åˆ†é’Ÿ
                        sock_connect=30,  # è¿æ¥è¶…æ—¶30ç§’
                        sock_read=120  # æ•°æ®è¯»å–è¶…æ—¶2åˆ†é’Ÿ
                    )
            ) as response:

                async for line in response.content:
                    line = line.strip()
                    # å¤„ç†æµç»“æŸæ ‡è®°
                    if line == b'data: [DONE]':
                        return
                    # è·³è¿‡éæ•°æ®è¡Œ
                    if not line.startswith(b'data: '):
                        continue
                    try:
                        chunk = json.loads(line[6:])  # ç§»é™¤ "data: " å‰ç¼€
                        # å®‰å…¨è®¿é—®ç»“æ„
                        try:
                            chunk = json.loads(line[6:])

                            # å¢å¼ºç©ºchoiceså¤„ç†
                            if not chunk.get('choices') or len(chunk['choices']) == 0:
                                logging.warning("ç©ºchoicesåˆ—è¡¨: %s", chunk)
                                yield "[APIå“åº”å¼‚å¸¸ï¼Œå·²è·³è¿‡æ— æ•ˆç‰‡æ®µ]"
                                continue  # å…³é”®ï¼šç«‹å³è·³è¿‡åç»­å¤„ç†

                            choice = chunk['choices'][0]
                            if 'delta' not in choice:
                                continue

                            content = choice['delta'].get('content', '')

                        except json.JSONDecodeError:
                            # å¤„ç†æ— æ•ˆJSON
                            logging.warning("æ— æ•ˆJSONæ•°æ®: %s", line)
                            yield "[æ•°æ®æ ¼å¼é”™è¯¯]"
                            continue
                        except IndexError as e:
                            logging.warning(f"æ— æ•ˆé€‰é¡¹ç´¢å¼•: {str(e)}")
                        except KeyError as e:
                            logging.warning(f"ç¼ºå¤±å…³é”®å­—æ®µ: {str(e)}")
                        if isinstance(content, str) and content.strip():  # è¿‡æ»¤ç©ºç™½å†…å®¹
                            yield content
                    except json.JSONDecodeError:
                        logging.warning("æ— æ•ˆJSONæ•°æ®: %s", line)
                    except KeyError:
                        logging.warning("å“åº”ç»“æ„å¼‚å¸¸: %s", chunk)
                    except aiohttp.ClientError as e:
                        logging.error(f"APIè¿æ¥å¤±è´¥: {str(e)}")
                        yield "[ç³»ç»Ÿé”™è¯¯ï¼šAPIè¿æ¥å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•]"
                        break  # é‡è¦ï¼šç»ˆæ­¢åç»­å¤„ç†
                    except Exception as e:
                        logging.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
                        yield "[ç³»ç»Ÿé”™è¯¯ï¼šæœåŠ¡æš‚æ—¶ä¸å¯ç”¨]"



def build_medical_engine(vector_db, embeddings):
    client = MedicalQwenClient()
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç”ŸåŠ©ç†ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š
{context}

å›ç­”è¦æ±‚ï¼š
1. å¿…é¡»ä»¥ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
   [è¯Šæ–­å»ºè®®]  
2. å¿…é¡»å®Œæ•´å¼•ç”¨å‚è€ƒèµ„æ–™ä¸­çš„åŸè¯ã€‚
3. è‹¥æ— ç›¸å…³å†…å®¹ï¼Œä¸¥æ ¼å›ç­”"æœªæ‰¾åˆ°åŒ»å­¦è¯æ®" """

    @lru_cache(maxsize=1000)
    # ä¿®æ”¹ cached_retrieval å‡½æ•°

    def cached_retrieval(question: str, top_k: int = 5):  # å¢åŠ top_kæ•°é‡
        # å¢å¼ºé—®é¢˜é¢„å¤„ç†
        processed_question = normalize_medical_terms(question)
        processed_question = re.sub(r"[^\w\u4e00-\u9fff]+", " ", processed_question)  # æ›´å¥½çš„ç‰¹æ®Šå­—ç¬¦å¤„ç†

        # ä½¿ç”¨æ··åˆæ£€ç´¢
        docs = vector_db.similarity_search(processed_question, k=top_k)
        scores = vector_db.similarity_search_with_score(processed_question, k=top_k)

        # åˆå¹¶ç»“æœ
        result_docs = []
        for doc, (_, score) in zip(docs, scores):
            doc.metadata.update({
                'similarity_score': float(1 - score),  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                'chunk_type': doc.metadata.get('chunk_type', 'other')
            })
            result_docs.append(doc)

        return result_docs

        # æ·»åŠ å…ƒæ•°æ®ä¿æŠ¤

    async def query_engine(question: str) -> AsyncGenerator[Dict, None]:
        """ä¿®æ”¹åçš„ç”Ÿæˆå™¨å‡½æ•°ï¼Œç›´æ¥ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬"""
        try:
            # 1. é—®é¢˜é¢„å¤„ç†
            processed_question = normalize_medical_terms(question)
            processed_question = _enhance_question_structure(processed_question)

            # 2. ç›´æ¥ä»æœ¬åœ°å‘é‡åº“æ£€ç´¢
            docs = cached_retrieval(processed_question)
            reranker = MedicalReranker()
            docs = await reranker.rerank(processed_question, docs)

            if not docs:
                yield {"error": "æœªæ‰¾åˆ°ç›¸å…³åŒ»å­¦æ–‡çŒ®"}
                return

            # 3. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¿®æ”¹ä¸ºç›´æ¥ç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼‰
            context_texts = []  # å­˜å‚¨çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡
            unique_sources = []  # ä¿ç•™ç»“æ„åŒ–æ¥æºä¿¡æ¯
            source_map = {}

            for idx, doc in enumerate(docs[:5], 1):  # é™åˆ¶æœ€å¤š5ä¸ªæ–‡æ¡£
                # ç¡®ä¿æ–‡æ¡£æœ‰å†…å®¹
                if not doc.page_content.strip():
                    continue

                # å¤„ç†æ ‡é¢˜å’Œé¡µç ï¼ˆç”¨äºç»“æ„åŒ–æ¥æºï¼‰
                title = doc.metadata.get('title', 'æœªçŸ¥æ–‡çŒ®')
                title = re.sub(r'[\n\r\t]', ' ', title)[:100].strip()
                page = str(doc.metadata.get('page', 'N/A')).strip()
                page = page if page.isdigit() else 'N/A'

                # æ„å»ºå”¯ä¸€é”®
                unique_key = f"{title}|{page}"

                # ç”Ÿæˆçº¯æ–‡æœ¬ä¸Šä¸‹æ–‡å†…å®¹ï¼ˆ500å­—ç¬¦é™åˆ¶ï¼‰
                doc_content = doc.page_content[:500].strip()
                context_entry = f"{doc_content} [æ¥æº: {title}, é¡µç : {page}]"

                if unique_key not in source_map:
                    source_map[unique_key] = {
                        "title": title,
                        "pages": set([page]) if page != 'N/A' else set(),
                        "content": doc_content  # æ–°å¢åŸå§‹å†…å®¹
                    }
                    context_texts.append(context_entry)
                else:
                    if page != 'N/A':
                        source_map[unique_key]["pages"].add(page)

            # å¤„ç†æœ€ç»ˆæ¥æºï¼ˆä¿æŒåŸæœ‰ç»“æ„ï¼‰
            for key in source_map:
                src = source_map[key]
                unique_sources.append({
                    "title": src['title'],
                    "pages": sorted(src['pages'], key=int) if src['pages'] else ['N/A'],
                    "content": src['content']  # æ–°å¢åŸå§‹å†…å®¹
                })

            # è°ƒè¯•æ—¥å¿—
            logging.info(f"ç”Ÿæˆ {len(context_texts)} æ¡ä¸Šä¸‹æ–‡æ–‡æœ¬")
            logging.debug(f"ç¤ºä¾‹ä¸Šä¸‹æ–‡æ–‡æœ¬:\n{context_texts[0][:200]}..." if context_texts else "æ— ä¸Šä¸‹æ–‡")

            # 4. æµå¼ç”Ÿæˆå›ç­”
            messages = [
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT.format(context="\n\n".join(context_texts))
                },
                {"role": "user", "content": f"ä¸´åºŠé—®é¢˜ï¼š{question}"}
            ]

            answer = []
            async for chunk in client.generate_stream(messages):
                if chunk:  # äºŒæ¬¡è¿‡æ»¤
                    # åŒæ—¶è¿”å›ä¸Šä¸‹æ–‡å—
                    yield {
                        "delta": chunk,
                        "context": "\n\n".join(context_texts)  # è¿”å›å®Œæ•´ä¸Šä¸‹æ–‡
                    }
                    answer.append(chunk)

            # æ˜ç¡®å‘é€æœ€ç»ˆç»“æœ
            full_answer = "".join([chunk for chunk in answer if isinstance(chunk, str)])
            if full_answer:
                yield {
                    "answer": full_answer,
                    "sources": unique_sources,
                    "context_texts": context_texts  # æ–°å¢çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡
                }
            else:
                yield {"error": "æ¨¡å‹æœªç”Ÿæˆæœ‰æ•ˆå“åº”"}

        except asyncio.CancelledError:
            logging.warning("ç”¨æˆ·å–æ¶ˆæŸ¥è¯¢")
            yield {"error": "æŸ¥è¯¢å·²å–æ¶ˆ"}
        except Exception as e:
            logging.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}", exc_info=True)
            yield {"error": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"}

    return MedicalEngine(query_engine)


async def interactive_query(engine):
    """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
    print("\n\033[1;36m=== åŒ»å­¦çŸ¥è¯†é—®ç­”æµ‹è¯•æ¨¡å¼ ===\033[0m")
    print("è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰:")

    while True:
        try:
            # è¯»å–ç”¨æˆ·è¾“å…¥
            question = input("\n\033[1;33m[æ‚¨çš„é—®é¢˜] > \033[0m").strip()
            if question.lower() in ["q", "quit", "exit"]:
                break
            if not question:
                continue

            # æ‰§è¡ŒæŸ¥è¯¢
            response = await engine.aquery(question)

            # æ ¼å¼åŒ–è¾“å‡º
            print("\n\033[1;32m[ç³»ç»Ÿå›ç­”]\033[0m")
            print(response["answer"])
            if response["references"]:
                print("\n\033[1;35mâ–² å‚è€ƒæ–‡çŒ®:\033[0m")
                print("\n".join(response["references"]))
            print("-" * 50)

        except (EOFError, KeyboardInterrupt):
            print("\né€€å‡ºäº¤äº’æ¨¡å¼")
            break
        except Exception as e:
            print(f"\n\033[1;31mé”™è¯¯: {str(e)}\033[0m")



# åˆå§‹åŒ–æµç¨‹
if __name__ == "__main__":
    load_dotenv()

    # åŠ è½½åŒ»å­¦ä¸“ç”¨å‘é‡åº“
    embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
    faiss_path = r"F:\biyesheji\rag1\data\medical_faiss"
    vector_db = FAISS.load_local(
        faiss_path,
        embeddings,
        allow_dangerous_deserialization=True
    )
    # æ ‡è®°æ ¸å¿ƒæ–‡æ¡£
    for doc in vector_db.docstore._dict.values():
        doc.metadata.setdefault('title', 'æœªå‘½åæ–‡çŒ®')
        doc.metadata['is_core'] = True

    # æ„å»ºå¼•æ“
    engine = build_medical_engine(vector_db, embeddings)

    # æµ‹è¯•ç¤ºä¾‹
    import asyncio
    from zbtool1 import RagasTester  # å¯¼å…¥æµ‹è¯•å·¥å…·ç±»

    # ä¿®æ”¹åçš„å…¥å£åˆ¤æ–­
    if "--test" in sys.argv:
        import argparse
        from pathlib import Path

        parser = argparse.ArgumentParser(description='åŒ»å­¦RAGæµ‹è¯•å·¥å…·')
        parser.add_argument('--test', action='store_true', help='å¯ç”¨æµ‹è¯•æ¨¡å¼')
        parser.add_argument('--file', type=str, help='æŒ‡å®šæµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹/ç»å¯¹è·¯å¾„ï¼‰')
        parser.add_argument('--csv', action='store_true', help='è¾“å‡ºCSVæ ¼å¼')
        args = parser.parse_args()

        try:
            # ========== è·¯å¾„ç»Ÿä¸€å¤„ç† ==========
            test_file = args.file or "test_questions.xlsx"

            # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
            tester = RagasTester(engine, test_file)

            # ========== å¼‚æ­¥æ‰§è¡Œä¼˜åŒ– ==========
            output_format = "csv" if args.csv else "json"


            async def _safe_run():
                """å®‰å…¨è¿è¡ŒåŒ…è£…å™¨"""
                try:
                    await tester.run(output_format)
                except RuntimeError as e:
                    if "Event loop" in str(e):
                        # Windowsç³»ç»Ÿç‰¹æ®Šå¤„ç†
                        policy = asyncio.WindowsSelectorEventLoopPolicy()
                        asyncio.set_event_loop_policy(policy)
                        await tester.run(output_format)
                    else:
                        raise
                except asyncio.CancelledError:
                    print("\næµ‹è¯•å·²ä¸­æ­¢")
                    sys.exit(130)


            # ========== æ‰§è¡Œæµ‹è¯• ==========
            try:
                asyncio.run(_safe_run())
            except KeyboardInterrupt:
                print("\n\033[33mæµ‹è¯•å·²æ‰‹åŠ¨ç»ˆæ­¢\033[0m")
                sys.exit(130)

        except FileNotFoundError as e:
            print(f"\033[31mERROR: æ–‡ä»¶æœªæ‰¾åˆ° - {e}\033[0m")
            sys.exit(1)
        except Exception as e:
            print(f"\033[31mæµ‹è¯•æµç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}\033[0m")
            sys.exit(1)

    else:
        # åŸæœ‰äº¤äº’æ¨¡å¼ä¿æŒä¸å˜
        asyncio.run(interactive_query(engine))



